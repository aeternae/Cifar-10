# Cifar-10
对CIFAR-10数据集的分类是机器学习中一个公开的基准测试问题，其任务是对一组32x32RGB的图像进行分类，这些图像涵盖了10个类别：飞机，汽车，鸟，猫，鹿，狗，青蛙，马，船以及卡车。

#### 为了熟悉掌握经典的DeepLearning Model以及Tensorflow的使用，我构建了多种模型对cifar10数据集进行分类。<br>
#### 在终端运行步骤如下：python Vgg19.py (以Vgg19模型为例）

```
!python Vgg19.py
Using TensorFlow backend.

======Loading data======
Loading ../input0/data_batch_1 : 10000.
Loading ../input0/data_batch_2 : 10000.
Loading ../input0/data_batch_3 : 10000.
Loading ../input0/data_batch_4 : 10000.
Loading ../input0/data_batch_5 : 10000.
Loading ../input0/test_batch : 10000.
Train data: (50000, 32, 32, 3) (50000, 10)
Test data : (10000, 32, 32, 3) (10000, 10)
======Load finished======
======Shuffling data======
======Prepare Finished======
2019-07-31 10:44:12.901688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-07-31 10:44:12.901769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 10:44:12.901797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-07-31 10:44:12.901813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-07-31 10:44:12.902583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9547 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:0e:00.0, compute capability: 7.5)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 64)        256       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 64)        0         
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 64)        256       
_________________________________________________________________
activation_2 (Activation)    (None, 32, 32, 64)        0         
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 128)       512       
_________________________________________________________________
activation_3 (Activation)    (None, 16, 16, 128)       0         
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    
_________________________________________________________________
batch_normalization_4 (Batch (None, 16, 16, 128)       512       
_________________________________________________________________
activation_4 (Activation)    (None, 16, 16, 128)       0         
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    
_________________________________________________________________
batch_normalization_5 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
activation_5 (Activation)    (None, 8, 8, 256)         0         
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
activation_6 (Activation)    (None, 8, 8, 256)         0         
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    
_________________________________________________________________
batch_normalization_7 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
activation_7 (Activation)    (None, 8, 8, 256)         0         
_________________________________________________________________
block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    
_________________________________________________________________
batch_normalization_8 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
activation_8 (Activation)    (None, 8, 8, 256)         0         
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   
_________________________________________________________________
batch_normalization_9 (Batch (None, 4, 4, 512)         2048      
_________________________________________________________________
activation_9 (Activation)    (None, 4, 4, 512)         0         
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   
_________________________________________________________________
batch_normalization_10 (Batc (None, 4, 4, 512)         2048      
_________________________________________________________________
activation_10 (Activation)   (None, 4, 4, 512)         0         
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   
_________________________________________________________________
batch_normalization_11 (Batc (None, 4, 4, 512)         2048      
_________________________________________________________________
activation_11 (Activation)   (None, 4, 4, 512)         0         
_________________________________________________________________
block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 4, 4, 512)         2048      
_________________________________________________________________
activation_12 (Activation)   (None, 4, 4, 512)         0         
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_13 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
activation_13 (Activation)   (None, 2, 2, 512)         0         
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_14 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
activation_14 (Activation)   (None, 2, 2, 512)         0         
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_15 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
activation_15 (Activation)   (None, 2, 2, 512)         0         
_________________________________________________________________
block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_16 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
activation_16 (Activation)   (None, 2, 2, 512)         0         
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
fc_cifa10 (Dense)            (None, 4096)              2101248   
_________________________________________________________________
batch_normalization_17 (Batc (None, 4096)              16384     
_________________________________________________________________
activation_17 (Activation)   (None, 4096)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
batch_normalization_18 (Batc (None, 4096)              16384     
_________________________________________________________________
activation_18 (Activation)   (None, 4096)              0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
predictions_cifa10 (Dense)   (None, 10)                40970     
_________________________________________________________________
batch_normalization_19 (Batc (None, 10)                40        
_________________________________________________________________
activation_19 (Activation)   (None, 10)                0         
=================================================================
Total params: 39,002,738
Trainable params: 38,975,326
Non-trainable params: 27,412
_________________________________________________________________
Using real-time data augmentation.
Epoch 1/100
391/391 [==============================] - 43s 111ms/step - loss: 2.4759 - acc: 0.5209 - val_loss: 3.6772 - val_acc: 0.4180
Epoch 2/100
391/391 [==============================] - 36s 92ms/step - loss: 2.0195 - acc: 0.6558 - val_loss: 2.1722 - val_acc: 0.6227
Epoch 3/100
391/391 [==============================] - 36s 92ms/step - loss: 1.8080 - acc: 0.7055 - val_loss: 2.3335 - val_acc: 0.5706
Epoch 4/100
391/391 [==============================] - 36s 92ms/step - loss: 1.6727 - acc: 0.7231 - val_loss: 1.7589 - val_acc: 0.6809
Epoch 5/100
391/391 [==============================] - 36s 92ms/step - loss: 1.5210 - acc: 0.7493 - val_loss: 1.6463 - val_acc: 0.6937
Epoch 6/100
391/391 [==============================] - 36s 91ms/step - loss: 1.4301 - acc: 0.7607 - val_loss: 1.9281 - val_acc: 0.6090
Epoch 7/100
391/391 [==============================] - 36s 91ms/step - loss: 1.3477 - acc: 0.7723 - val_loss: 1.5058 - val_acc: 0.7201
Epoch 8/100
391/391 [==============================] - 36s 92ms/step - loss: 1.2675 - acc: 0.7809 - val_loss: 1.6144 - val_acc: 0.6805
Epoch 9/100
391/391 [==============================] - 36s 92ms/step - loss: 1.2028 - acc: 0.7910 - val_loss: 2.0555 - val_acc: 0.5970
Epoch 10/100
391/391 [==============================] - 36s 91ms/step - loss: 1.1809 - acc: 0.7904 - val_loss: 1.4655 - val_acc: 0.6751
Epoch 11/100
391/391 [==============================] - 36s 91ms/step - loss: 1.1135 - acc: 0.8025 - val_loss: 1.3481 - val_acc: 0.7392
Epoch 12/100
391/391 [==============================] - 36s 91ms/step - loss: 1.0885 - acc: 0.8051 - val_loss: 1.3639 - val_acc: 0.7165
Epoch 13/100
391/391 [==============================] - 36s 91ms/step - loss: 1.0441 - acc: 0.8154 - val_loss: 1.6863 - val_acc: 0.6459
Epoch 14/100
391/391 [==============================] - 36s 91ms/step - loss: 1.0316 - acc: 0.8115 - val_loss: 1.2171 - val_acc: 0.7516
Epoch 15/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9920 - acc: 0.8232 - val_loss: 1.4487 - val_acc: 0.6709
Epoch 16/100
391/391 [==============================] - 36s 91ms/step - loss: 1.0010 - acc: 0.8189 - val_loss: 1.1901 - val_acc: 0.7541
Epoch 17/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9600 - acc: 0.8264 - val_loss: 1.1994 - val_acc: 0.7518
Epoch 18/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9738 - acc: 0.8230 - val_loss: 1.7142 - val_acc: 0.6390
Epoch 19/100
391/391 [==============================] - 35s 91ms/step - loss: 0.9291 - acc: 0.8356 - val_loss: 1.0710 - val_acc: 0.7861
Epoch 20/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9499 - acc: 0.8284 - val_loss: 1.5056 - val_acc: 0.7129
Epoch 21/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9128 - acc: 0.8398 - val_loss: 1.1143 - val_acc: 0.7728
Epoch 22/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9230 - acc: 0.8366 - val_loss: 1.1539 - val_acc: 0.7747
Epoch 23/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9049 - acc: 0.8420 - val_loss: 1.3152 - val_acc: 0.7428
Epoch 24/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9225 - acc: 0.8394 - val_loss: 1.2619 - val_acc: 0.7318
Epoch 25/100
391/391 [==============================] - 36s 91ms/step - loss: 0.8864 - acc: 0.8513 - val_loss: 1.3627 - val_acc: 0.7204
Epoch 26/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9138 - acc: 0.8444 - val_loss: 1.0893 - val_acc: 0.7811
Epoch 27/100
391/391 [==============================] - 36s 91ms/step - loss: 0.8798 - acc: 0.8554 - val_loss: 1.2733 - val_acc: 0.7195
Epoch 28/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9034 - acc: 0.8482 - val_loss: 1.4578 - val_acc: 0.6974
Epoch 29/100
391/391 [==============================] - 36s 91ms/step - loss: 0.8811 - acc: 0.8541 - val_loss: 1.1119 - val_acc: 0.7789
Epoch 30/100
391/391 [==============================] - 36s 91ms/step - loss: 0.9065 - acc: 0.8475 - val_loss: 2.5542 - val_acc: 0.4741
Epoch 31/100
391/391 [==============================] - 36s 91ms/step - loss: 0.8828 - acc: 0.8551 - val_loss: 1.4319 - val_acc: 0.7045
Epoch 32/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8945 - acc: 0.8530 - val_loss: 1.0702 - val_acc: 0.8026
Epoch 33/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8876 - acc: 0.8567 - val_loss: 1.0422 - val_acc: 0.8098
Epoch 34/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8961 - acc: 0.8552 - val_loss: 1.1375 - val_acc: 0.7852
Epoch 35/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8726 - acc: 0.8606 - val_loss: 1.3114 - val_acc: 0.7513
Epoch 36/100
391/391 [==============================] - 36s 91ms/step - loss: 0.8987 - acc: 0.8572 - val_loss: 1.1814 - val_acc: 0.7697
Epoch 37/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8849 - acc: 0.8623 - val_loss: 1.3548 - val_acc: 0.7201
Epoch 38/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8991 - acc: 0.8597 - val_loss: 1.1565 - val_acc: 0.7817
Epoch 39/100
391/391 [==============================] - 36s 92ms/step - loss: 0.8842 - acc: 0.8646 - val_loss: 1.0469 - val_acc: 0.8061
Epoch 40/100
391/391 [==============================] - 36s 92ms/step - loss: 0.9021 - acc: 0.8587 - val_loss: 1.3932 - val_acc: 0.7255
Epoch 41/100
391/391 [==============================] - 36s 91ms/step - loss: 0.7605 - acc: 0.9055 - val_loss: 0.7895 - val_acc: 0.8920
Epoch 42/100
391/391 [==============================] - 36s 92ms/step - loss: 0.6987 - acc: 0.9221 - val_loss: 0.7598 - val_acc: 0.8994
Epoch 43/100
391/391 [==============================] - 36s 92ms/step - loss: 0.6701 - acc: 0.9298 - val_loss: 0.7437 - val_acc: 0.9013
Epoch 44/100
391/391 [==============================] - 36s 91ms/step - loss: 0.6482 - acc: 0.9333 - val_loss: 0.7310 - val_acc: 0.9050
Epoch 45/100
391/391 [==============================] - 36s 92ms/step - loss: 0.6179 - acc: 0.9416 - val_loss: 0.7202 - val_acc: 0.9048
Epoch 46/100
391/391 [==============================] - 36s 92ms/step - loss: 0.6216 - acc: 0.9389 - val_loss: 0.7084 - val_acc: 0.9090
Epoch 47/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5981 - acc: 0.9432 - val_loss: 0.7249 - val_acc: 0.9008
Epoch 48/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5883 - acc: 0.9434 - val_loss: 0.6901 - val_acc: 0.9095
Epoch 49/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5661 - acc: 0.9499 - val_loss: 0.7019 - val_acc: 0.9067
Epoch 50/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5672 - acc: 0.9499 - val_loss: 0.6729 - val_acc: 0.9100
Epoch 51/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5431 - acc: 0.9556 - val_loss: 0.6837 - val_acc: 0.9073
Epoch 52/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5466 - acc: 0.9512 - val_loss: 0.6843 - val_acc: 0.9073
Epoch 53/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5215 - acc: 0.9580 - val_loss: 0.6812 - val_acc: 0.9063
Epoch 54/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5174 - acc: 0.9569 - val_loss: 0.6679 - val_acc: 0.9075
Epoch 55/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5030 - acc: 0.9612 - val_loss: 0.6617 - val_acc: 0.9098
Epoch 56/100
391/391 [==============================] - 36s 92ms/step - loss: 0.5044 - acc: 0.9587 - val_loss: 0.6463 - val_acc: 0.9153
Epoch 57/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4856 - acc: 0.9612 - val_loss: 0.6592 - val_acc: 0.9100
Epoch 58/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4799 - acc: 0.9628 - val_loss: 0.6489 - val_acc: 0.9096
Epoch 59/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4632 - acc: 0.9677 - val_loss: 0.6535 - val_acc: 0.9074
Epoch 60/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4602 - acc: 0.9667 - val_loss: 0.6477 - val_acc: 0.9083
Epoch 61/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4501 - acc: 0.9684 - val_loss: 0.6541 - val_acc: 0.9050
Epoch 62/100
391/391 [==============================] - 36s 91ms/step - loss: 0.4517 - acc: 0.9663 - val_loss: 0.6828 - val_acc: 0.8999
Epoch 63/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4410 - acc: 0.9674 - val_loss: 0.6434 - val_acc: 0.9076
Epoch 64/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4385 - acc: 0.9672 - val_loss: 0.6492 - val_acc: 0.9058
Epoch 65/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4243 - acc: 0.9709 - val_loss: 0.6395 - val_acc: 0.9056
Epoch 66/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4229 - acc: 0.9691 - val_loss: 0.6654 - val_acc: 0.9029
Epoch 67/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4125 - acc: 0.9721 - val_loss: 0.6180 - val_acc: 0.9105
Epoch 68/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4164 - acc: 0.9690 - val_loss: 0.6507 - val_acc: 0.9060
Epoch 69/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3999 - acc: 0.9743 - val_loss: 0.6284 - val_acc: 0.9074
Epoch 70/100
391/391 [==============================] - 36s 92ms/step - loss: 0.4051 - acc: 0.9709 - val_loss: 0.6421 - val_acc: 0.9071
Epoch 71/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3872 - acc: 0.9754 - val_loss: 0.6219 - val_acc: 0.9093
Epoch 72/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3908 - acc: 0.9731 - val_loss: 0.6422 - val_acc: 0.9055
Epoch 73/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3774 - acc: 0.9756 - val_loss: 0.6662 - val_acc: 0.9003
Epoch 74/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3843 - acc: 0.9725 - val_loss: 0.6446 - val_acc: 0.8994
Epoch 75/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3773 - acc: 0.9744 - val_loss: 0.6051 - val_acc: 0.9096
Epoch 76/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3720 - acc: 0.9752 - val_loss: 0.6142 - val_acc: 0.9087
Epoch 77/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3630 - acc: 0.9755 - val_loss: 0.6232 - val_acc: 0.9082
Epoch 78/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3635 - acc: 0.9761 - val_loss: 0.5953 - val_acc: 0.9096
Epoch 79/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3578 - acc: 0.9770 - val_loss: 0.6529 - val_acc: 0.8987
Epoch 80/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3582 - acc: 0.9759 - val_loss: 0.6730 - val_acc: 0.8971
Epoch 81/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3381 - acc: 0.9818 - val_loss: 0.5735 - val_acc: 0.9170
Epoch 82/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3290 - acc: 0.9841 - val_loss: 0.5709 - val_acc: 0.9180
Epoch 83/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3215 - acc: 0.9881 - val_loss: 0.5702 - val_acc: 0.9166
Epoch 84/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3220 - acc: 0.9870 - val_loss: 0.5699 - val_acc: 0.9173
Epoch 85/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3143 - acc: 0.9898 - val_loss: 0.5698 - val_acc: 0.9193
Epoch 86/100
391/391 [==============================] - 36s 91ms/step - loss: 0.3168 - acc: 0.9883 - val_loss: 0.5731 - val_acc: 0.9181
Epoch 87/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3142 - acc: 0.9894 - val_loss: 0.5704 - val_acc: 0.9184
Epoch 88/100
391/391 [==============================] - 36s 91ms/step - loss: 0.3117 - acc: 0.9900 - val_loss: 0.5758 - val_acc: 0.9194
Epoch 89/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3081 - acc: 0.9912 - val_loss: 0.5745 - val_acc: 0.9186
Epoch 90/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3105 - acc: 0.9898 - val_loss: 0.5784 - val_acc: 0.9190
Epoch 91/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3079 - acc: 0.9919 - val_loss: 0.5784 - val_acc: 0.9184
Epoch 92/100
391/391 [==============================] - 36s 91ms/step - loss: 0.3069 - acc: 0.9914 - val_loss: 0.5821 - val_acc: 0.9174
Epoch 93/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3038 - acc: 0.9913 - val_loss: 0.5785 - val_acc: 0.9182
Epoch 94/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3077 - acc: 0.9905 - val_loss: 0.5815 - val_acc: 0.9187
Epoch 95/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3018 - acc: 0.9921 - val_loss: 0.5834 - val_acc: 0.9189
Epoch 96/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3043 - acc: 0.9918 - val_loss: 0.5801 - val_acc: 0.9197
Epoch 97/100
391/391 [==============================] - 36s 92ms/step - loss: 0.2996 - acc: 0.9932 - val_loss: 0.5792 - val_acc: 0.9219
Epoch 98/100
391/391 [==============================] - 36s 92ms/step - loss: 0.2986 - acc: 0.9927 - val_loss: 0.5791 - val_acc: 0.9209
Epoch 99/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3000 - acc: 0.9920 - val_loss: 0.5818 - val_acc: 0.9206
Epoch 100/100
391/391 [==============================] - 36s 92ms/step - loss: 0.3003 - acc: 0.9918 - val_loss: 0.5815 - val_acc: 0.9203
​
